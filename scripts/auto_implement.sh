#!/bin/bash
# auto_implement.sh - Ëá™Âä®Âåñ Scalar ËøÅÁßªÂÆûÊñΩ
# ÊØè‰∏™Èò∂ÊÆµÂåÖÂê´: ÂÆûÊñΩ ‚Üí 2ËΩÆ‰∏•Ê†ºÊµãËØï ‚Üí ‰ª£Á†ÅÊâ´Êèè ‚Üí ÊñáÊ°£Êõ¥Êñ∞ ‚Üí Git Êèê‰∫§

set -e  # ÈÅáÂà∞ÈîôËØØÁ´ãÂç≥ÈÄÄÂá∫

# È¢úËâ≤ÂÆö‰πâ
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# È°πÁõÆÊ†πÁõÆÂΩï
PROJECT_ROOT="/Users/yemuyu/Documents/Yemu Yu/00 Project/13 arxiv-paper-curator"
cd "$PROJECT_ROOT"

# Êó•ÂøóÊñá‰ª∂
LOG_DIR="logs/implementation"
mkdir -p "$LOG_DIR"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_FILE="$LOG_DIR/implementation_${TIMESTAMP}.log"

# Êó•ÂøóÂáΩÊï∞
log() {
    echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1" | tee -a "$LOG_FILE"
}

log_success() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] ‚úÖ $1${NC}" | tee -a "$LOG_FILE"
}

log_error() {
    echo -e "${RED}[$(date +'%Y-%m-%d %H:%M:%S')] ‚ùå $1${NC}" | tee -a "$LOG_FILE"
}

log_warning() {
    echo -e "${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] ‚ö†Ô∏è  $1${NC}" | tee -a "$LOG_FILE"
}

# ÈîôËØØÂ§ÑÁêÜ
trap 'log_error "Script failed at line $LINENO. Check $LOG_FILE for details."; exit 1' ERR

# ÊøÄÊ¥ªËôöÊãüÁéØÂ¢É
if [ -f ".venv/bin/activate" ]; then
    source .venv/bin/activate
    log "‚úÖ Virtual environment activated"
fi

# ============================================================================
# ËæÖÂä©ÂáΩÊï∞
# ============================================================================

# ËøêË°åÊµãËØïÂπ∂Êî∂ÈõÜÁªìÊûú
run_test_round() {
    local round=$1
    local test_type=$2

    log "üß™ Running Test Round $round: $test_type"

    local test_result=0

    case $test_type in
        "unit")
            # ÂçïÂÖÉÊµãËØï (Â¶ÇÊûúÂ≠òÂú®)
            if [ -f "tests/test_openapi_compliance.py" ]; then
                pytest tests/test_openapi_compliance.py -v --tb=short >> "$LOG_FILE" 2>&1 || test_result=$?
            fi
            ;;
        "integration")
            # ÈõÜÊàêÊµãËØï
            if [ -f "scripts/acceptance_test_v2.sh" ]; then
                bash scripts/acceptance_test_v2.sh >> "$LOG_FILE" 2>&1 || test_result=$?
            fi
            ;;
        "validation")
            # OpenAPI È™åËØÅ
            if [ -f "scripts/validate_openapi.sh" ]; then
                bash scripts/validate_openapi.sh >> "$LOG_FILE" 2>&1 || test_result=$?
            fi
            ;;
        "security")
            # ÂÆâÂÖ®Êâ´Êèè
            if [ -f "scripts/security_audit.sh" ]; then
                bash scripts/security_audit.sh >> "$LOG_FILE" 2>&1 || test_result=$?
            fi
            ;;
    esac

    if [ $test_result -eq 0 ]; then
        log_success "Test Round $round ($test_type) PASSED"
        return 0
    else
        log_error "Test Round $round ($test_type) FAILED"
        return 1
    fi
}

# 2ËΩÆ‰∏•Ê†ºÊµãËØï
run_strict_testing() {
    local stage=$1

    log "üéØ Starting 2-Round Strict Testing for $stage"

    # Round 1: Âø´ÈÄüÈ™åËØÅ
    log "Round 1: Quick Validation"
    run_test_round 1 "validation" || {
        log_error "Round 1 failed. Aborting implementation."
        return 1
    }

    # Round 2: ÂÆåÊï¥ÊµãËØï
    log "Round 2: Comprehensive Testing"
    run_test_round 2 "integration" || {
        log_error "Round 2 failed. Aborting implementation."
        return 1
    }

    log_success "All 2 test rounds passed for $stage"
    return 0
}

# Êâ´ÊèèÂπ∂Âà†Èô§Êó†ÂÖ≥‰ª£Á†Å
scan_and_clean() {
    log "üßπ Scanning for unnecessary code..."

    # Âà†Èô§ __pycache__
    find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
    log "  Removed __pycache__ directories"

    # Âà†Èô§ .pyc Êñá‰ª∂
    find . -type f -name "*.pyc" -delete 2>/dev/null || true
    log "  Removed .pyc files"

    # Âà†Èô§‰∏¥Êó∂Êñá‰ª∂
    find . -type f -name "*.tmp" -delete 2>/dev/null || true
    find . -type f -name ".DS_Store" -delete 2>/dev/null || true
    log "  Removed temporary files"

    # Ê£ÄÊü•Êú™‰ΩøÁî®ÁöÑÂØºÂÖ• (Â¶ÇÊûúÂÆâË£Ö‰∫Ü ruff)
    if command -v ruff &> /dev/null; then
        log "  Running ruff to check unused imports..."
        ruff check src/ --select F401 --fix >> "$LOG_FILE" 2>&1 || true
    fi

    log_success "Code cleanup completed"
}

# Êõ¥Êñ∞ CLAUDE.md
update_claude_md() {
    local stage=$1
    local description=$2

    log "üìù Updating CLAUDE.md for $stage"

    # Â§á‰ªΩÂéüÊñá‰ª∂
    cp CLAUDE.md "CLAUDE.md.backup_${TIMESTAMP}"

    # Ê∑ªÂä†Êõ¥Êñ∞ËÆ∞ÂΩï
    cat >> CLAUDE.md <<EOF

---

## üîÑ Scalar Migration Progress

### $stage - $(date +"%Y-%m-%d %H:%M:%S")

**Status**: ‚úÖ Completed

**Changes**: $description

**Tests**: All 2 test rounds passed

**Files Modified**:
$(git status --short | grep -E "^M|^A" | sed 's/^/  - /')

**Next Step**: $(get_next_stage "$stage")

EOF

    log_success "CLAUDE.md updated"
}

# Ëé∑Âèñ‰∏ã‰∏ÄÈò∂ÊÆµ
get_next_stage() {
    local current=$1

    case $current in
        "Day 0: Pre-Implementation")
            echo "Day 1: Environment Setup and Baseline Testing"
            ;;
        "Day 1: Environment Setup and Baseline Testing")
            echo "Day 2: OpenAPI Specification Enhancement"
            ;;
        "Day 2: OpenAPI Specification Enhancement")
            echo "Day 3: Scalar Static Site Generation"
            ;;
        "Day 3: Scalar Static Site Generation")
            echo "Day 4: SSE Endpoint Optimization and Testing"
            ;;
        *)
            echo "Continue with remaining days"
            ;;
    esac
}

# Git Êèê‰∫§ÂíåÊé®ÈÄÅ
git_commit_and_push() {
    local stage=$1
    local description=$2

    log "üì¶ Committing changes for $stage"

    # Ê∑ªÂä†ÊâÄÊúâ‰øÆÊîπÁöÑÊñá‰ª∂
    git add -A

    # Ê£ÄÊü•ÊòØÂê¶ÊúâÂèòÊõ¥
    if git diff --cached --quiet; then
        log_warning "No changes to commit for $stage"
        return 0
    fi

    # ÂàõÂª∫Êèê‰∫§
    local commit_message="$(cat <<EOF
$stage

$description

- ‚úÖ All 2 test rounds passed
- ‚úÖ Code cleanup completed
- ‚úÖ Documentation updated

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
EOF
)"

    git commit -m "$commit_message" >> "$LOG_FILE" 2>&1

    log_success "Changes committed"

    # Êé®ÈÄÅÂà∞ËøúÁ®ã (ÂèØÈÄâ,ÈÄöËøáÂèÇÊï∞ÊéßÂà∂)
    if [ "${AUTO_PUSH:-true}" = "true" ]; then
        log "üì§ Pushing to remote repository..."
        git push origin main >> "$LOG_FILE" 2>&1 || {
            log_warning "Push failed. You may need to push manually."
            return 1
        }
        log_success "Changes pushed to GitHub"
    else
        log_warning "Auto-push disabled. Run 'git push' manually."
    fi
}

# ============================================================================
# ÂÆûÊñΩÈò∂ÊÆµ
# ============================================================================

# Pre-Implementation: ‰æùËµñÂÆâË£Ö
pre_implementation() {
    log "üöÄ Starting Pre-Implementation: Dependency Installation"

    # Ê£ÄÊü•Âπ∂ÂÆâË£Ö‰æùËµñ
    log "Installing missing dependencies..."

    # ÂÆâË£Ö Python ÊµãËØï‰æùËµñ
    if ! python -c "import pytest" 2>/dev/null; then
        log "Installing pytest..."
        uv sync --group dev >> "$LOG_FILE" 2>&1 || pip install pytest pytest-asyncio >> "$LOG_FILE" 2>&1
    fi

    # Ê£ÄÊü• jq
    if ! command -v jq &> /dev/null; then
        log_warning "jq not installed. Installing..."
        brew install jq >> "$LOG_FILE" 2>&1 || {
            log_error "Failed to install jq. Please install manually."
            return 1
        }
    fi

    # È™åËØÅÂÆâË£Ö
    python -c "import pytest; import httpx; print('‚úÖ Python deps OK')" || {
        log_error "Python dependencies verification failed"
        return 1
    }

    jq --version >> "$LOG_FILE" 2>&1 || {
        log_error "jq verification failed"
        return 1
    }

    log_success "All dependencies installed"

    # 2ËΩÆÊµãËØï (È™åËØÅÁéØÂ¢É)
    log "Testing environment readiness..."

    # Round 1: Python ÁéØÂ¢É
    python --version >> "$LOG_FILE" 2>&1
    python -c "import fastapi; import pydantic; import httpx" || {
        log_error "Python environment test failed"
        return 1
    }
    log_success "Round 1: Python environment OK"

    # Round 2: Á≥ªÁªüÂ∑•ÂÖ∑
    curl --version >> "$LOG_FILE" 2>&1
    jq --version >> "$LOG_FILE" 2>&1
    npm --version >> "$LOG_FILE" 2>&1
    log_success "Round 2: System tools OK"

    # ‰ª£Á†ÅÊ∏ÖÁêÜ
    scan_and_clean

    # Êõ¥Êñ∞ÊñáÊ°£
    update_claude_md "Day 0: Pre-Implementation" "Installed all dependencies and verified environment"

    # Git Êèê‰∫§
    git_commit_and_push "Day 0: Pre-Implementation Setup" "Install dependencies: pytest, pytest-asyncio, jq"

    log_success "Pre-Implementation completed"
}

# Day 1: ÁéØÂ¢ÉÂáÜÂ§áÂíåÂü∫Á∫øÊµãËØï
implement_day1() {
    log "üöÄ Starting Day 1: Environment Setup and Baseline Testing"

    # ÂàõÂª∫ËÑöÊú¨ÁõÆÂΩï
    mkdir -p scripts tests

    # ÂàõÂª∫Âü∫Á∫øÊÄßËÉΩÊµãËØïËÑöÊú¨
    log "Creating baseline performance test script..."

    cat > scripts/baseline_performance.py <<'PYTHON_SCRIPT'
#!/usr/bin/env python3
"""Performance baseline test for API before Scalar migration"""

import asyncio
import time
import statistics
from typing import List, Dict
import httpx
import json

BASE_URL = "http://localhost:8000"

async def test_endpoint_latency(endpoint: str, method: str = "GET", json_data: dict = None) -> float:
    async with httpx.AsyncClient(timeout=30.0) as client:
        start = time.time()
        if method == "GET":
            response = await client.get(f"{BASE_URL}{endpoint}")
        else:
            response = await client.post(f"{BASE_URL}{endpoint}", json=json_data)
        latency = time.time() - start
        if response.status_code != 200:
            raise Exception(f"Request failed: {response.status_code}")
        return latency

async def benchmark_endpoint(endpoint: str, method: str = "GET", json_data: dict = None, iterations: int = 10) -> Dict:
    latencies = []
    for i in range(iterations):
        try:
            latency = await test_endpoint_latency(endpoint, method, json_data)
            latencies.append(latency)
            await asyncio.sleep(0.5)
        except Exception as e:
            print(f"  ‚ùå Iteration {i+1} failed: {e}")

    if not latencies:
        return {"error": "All requests failed"}

    return {
        "endpoint": endpoint,
        "method": method,
        "iterations": len(latencies),
        "min": min(latencies),
        "max": max(latencies),
        "mean": statistics.mean(latencies),
        "median": statistics.median(latencies),
        "stdev": statistics.stdev(latencies) if len(latencies) > 1 else 0,
        "p95": sorted(latencies)[int(len(latencies) * 0.95)] if len(latencies) > 1 else latencies[0],
    }

async def main():
    print("üöÄ Starting Performance Baseline Test...")
    print("=" * 60)

    test_cases = [
        {"name": "Health Check", "endpoint": "/api/v1/health", "method": "GET"},
        {"name": "OpenAPI Spec", "endpoint": "/openapi.json", "method": "GET"},
        {"name": "Hybrid Search", "endpoint": "/api/v1/hybrid-search/", "method": "POST",
         "json": {"query": "transformer architecture", "size": 5, "use_hybrid": True}},
    ]

    results = []
    for test in test_cases:
        print(f"\nüìä Testing: {test['name']}")
        print(f"   Endpoint: {test['method']} {test['endpoint']}")
        result = await benchmark_endpoint(endpoint=test['endpoint'], method=test['method'],
                                         json_data=test.get('json'), iterations=10)
        if "error" not in result:
            print(f"   ‚úÖ Mean: {result['mean']*1000:.0f}ms | P95: {result['p95']*1000:.0f}ms")
        results.append({**test, **result})

    with open("baseline_performance.json", "w") as f:
        json.dump(results, f, indent=2)

    print("\n" + "=" * 60)
    print("‚úÖ Baseline test complete!")
    print("üìÅ Results saved to: baseline_performance.json")

if __name__ == "__main__":
    asyncio.run(main())
PYTHON_SCRIPT

    chmod +x scripts/baseline_performance.py

    # Ê£ÄÊü• API ÊòØÂê¶ËøêË°å
    log "Checking if API is running..."
    if ! curl -f -s http://localhost:8000/api/v1/health > /dev/null 2>&1; then
        log_warning "API not running. Starting Docker Compose..."
        docker compose up -d api >> "$LOG_FILE" 2>&1
        log "Waiting for API to be healthy..."
        sleep 15
    fi

    # ËøêË°åÂü∫Á∫øÊµãËØï
    log "Running baseline performance test..."
    python scripts/baseline_performance.py >> "$LOG_FILE" 2>&1 || {
        log_error "Baseline test failed"
        return 1
    }

    # ÂØºÂá∫ÂéüÂßã OpenAPI spec
    log "Exporting original OpenAPI spec..."
    curl -s http://localhost:8000/openapi.json | jq . > openapi_v1_original.json

    # 2ËΩÆ‰∏•Ê†ºÊµãËØï
    log_success "Day 1 implementation completed"

    # Round 1: È™åËØÅÂü∫Á∫øÊñá‰ª∂
    if [ ! -f "baseline_performance.json" ] || [ ! -f "openapi_v1_original.json" ]; then
        log_error "Round 1: Required files missing"
        return 1
    fi
    log_success "Round 1: Files created successfully"

    # Round 2: È™åËØÅ API ÂÅ•Â∫∑
    curl -f -s http://localhost:8000/api/v1/health > /dev/null 2>&1 || {
        log_error "Round 2: API health check failed"
        return 1
    }
    log_success "Round 2: API health verified"

    # ‰ª£Á†ÅÊ∏ÖÁêÜ
    scan_and_clean

    # Êõ¥Êñ∞ÊñáÊ°£
    update_claude_md "Day 1: Environment Setup and Baseline Testing" \
        "Created baseline performance test and exported original OpenAPI spec"

    # Git Êèê‰∫§
    git_commit_and_push "Day 1: Baseline Testing Complete" \
        "Add baseline performance test script and export original OpenAPI spec"

    log_success "Day 1 completed successfully"
}

# Day 2: OpenAPI ËßÑËåÉÂ¢ûÂº∫
implement_day2() {
    log "üöÄ Starting Day 2: OpenAPI Specification Enhancement"

    # Â§á‰ªΩÂéüÊñá‰ª∂
    cp src/main.py "src/main.py.backup_${TIMESTAMP}"
    cp src/routers/ask.py "src/routers/ask.py.backup_${TIMESTAMP}"

    log "Modifying src/main.py..."

    # Ê≥®ÊÑè: ËøôÈáåÊàë‰ª¨‰∏çÁõ¥Êé•‰øÆÊîπÊñá‰ª∂,ËÄåÊòØÂàõÂª∫‰∏Ä‰∏™ÂæÖÂÆ°Ê†∏ÁöÑË°•‰∏Å
    # ÂÆûÈôÖ‰øÆÊîπÈúÄË¶Å‰∫∫Â∑•Á°ÆËÆ§‰ª•ÈÅøÂÖçÁ†¥ÂùèÁé∞Êúâ‰ª£Á†Å

    log_warning "Day 2 requires manual code modifications"
    log "Please manually apply changes from SCALAR_IMPLEMENTATION_GUIDE_V2.md Day 2"
    log "After modifications, run: ./scripts/auto_implement.sh --continue-day2"

    return 0
}

# Day 3: Scalar ÈùôÊÄÅÁ´ôÁÇπÁîüÊàê
implement_day3() {
    log "üöÄ Starting Day 3: Scalar Static Site Generation"

    # Á°Æ‰øù static ÁõÆÂΩïÂ≠òÂú®
    mkdir -p static

    log "Creating Scalar HTML documentation..."

    # ÂàõÂª∫ api-docs.html (‰ªé V2 guide Â§çÂà∂ÂÆåÊï¥ÂÜÖÂÆπ)
    cat > static/api-docs.html <<'HTML_CONTENT'
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Curator API Documentation</title>
    <style>
        body { margin: 0; padding: 0; }
    </style>
</head>
<body>
    <script
        id="api-reference"
        type="application/json"
        data-url="http://localhost:8000/openapi.json">
    </script>

    <script src="https://cdn.jsdelivr.net/npm/@scalar/api-reference@1.25.30/dist/browser/standalone.min.js"></script>

    <script>
        const configuration = {
            spec: { url: 'http://localhost:8000/openapi.json' },
            theme: 'purple',
            layout: 'modern',
            darkMode: false,
            showSidebar: true,
            hideDownloadButton: false,
            hideTestRequestSnippets: false,
            defaultHttpClient: {
                targetKey: 'javascript',
                clientKey: 'fetch'
            }
        };
    </script>
</body>
</html>
HTML_CONTENT

    log_success "Scalar HTML created"

    # 2ËΩÆÊµãËØï
    log "Round 1: Verify file creation"
    [ -f "static/api-docs.html" ] || {
        log_error "static/api-docs.html not created"
        return 1
    }
    log_success "Round 1: File verified"

    log "Round 2: Validate HTML syntax"
    grep -q "scalar/api-reference" static/api-docs.html || {
        log_error "Invalid HTML content"
        return 1
    }
    log_success "Round 2: HTML validated"

    # ‰ª£Á†ÅÊ∏ÖÁêÜ
    scan_and_clean

    # Êõ¥Êñ∞ÊñáÊ°£
    update_claude_md "Day 3: Scalar Static Site Generation" \
        "Created Scalar API documentation HTML with CDN integration"

    # Git Êèê‰∫§
    git_commit_and_push "Day 3: Scalar UI Complete" \
        "Add static/api-docs.html for Scalar API documentation"

    log_success "Day 3 completed successfully"
}

# ============================================================================
# ‰∏ªÊµÅÁ®ã
# ============================================================================

main() {
    log "========================================="
    log "ü§ñ Automated Scalar Migration Implementation"
    log "========================================="
    log "Project: arXiv Paper Curator"
    log "Repository: https://github.com/Yemu-Yu/arxiv-paper-curator"
    log "Log file: $LOG_FILE"
    log "========================================="

    # Ëß£ÊûêÂëΩ‰ª§Ë°åÂèÇÊï∞
    STAGE=${1:-"all"}

    case $STAGE in
        "all")
            log "Running full implementation (Day 0 - Day 3)"
            pre_implementation || exit 1
            implement_day1 || exit 1
            implement_day2 || exit 1
            implement_day3 || exit 1
            ;;
        "day0"|"pre")
            pre_implementation || exit 1
            ;;
        "day1")
            implement_day1 || exit 1
            ;;
        "day2")
            implement_day2 || exit 1
            ;;
        "day3")
            implement_day3 || exit 1
            ;;
        *)
            log_error "Unknown stage: $STAGE"
            echo "Usage: $0 [all|day0|day1|day2|day3]"
            exit 1
            ;;
    esac

    log "========================================="
    log_success "üéâ Implementation stage '$STAGE' completed successfully!"
    log "üìä View logs: $LOG_FILE"
    log "üîó GitHub: https://github.com/Yemu-Yu/arxiv-paper-curator"
    log "========================================="
}

# ÊâßË°å‰∏ªÊµÅÁ®ã
main "$@"
