# arXiv Paper Curator - å®Œæ•´ API æ–‡æ¡£

## ğŸ“‹ API æ¦‚è§ˆ

è¯¥ RAG ç³»ç»Ÿæä¾› **6 ä¸ªæ ¸å¿ƒ API ç«¯ç‚¹**ï¼Œåˆ†ä¸º 4 ä¸ªåŠŸèƒ½æ¨¡å—ï¼š

| æ¨¡å— | ç«¯ç‚¹æ•°é‡ | ç”¨é€” |
|------|---------|------|
| **Health Check** | 1 | ç³»ç»Ÿå¥åº·ç›‘æ§ |
| **Search** | 1 | æ–‡æ¡£æ··åˆæœç´¢ |
| **Basic RAG** | 2 | åŸºç¡€é—®ç­”ï¼ˆåŒæ­¥/æµå¼ï¼‰ |
| **Agentic RAG** | 2 | æ™ºèƒ½é—®ç­” + åé¦ˆ |

---

## ğŸ—ï¸ API æ¶æ„å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FastAPI Application                      â”‚
â”‚                  (http://localhost:8000)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
        â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Health Check â”‚   â”‚    Search    â”‚   â”‚     RAG      â”‚
â”‚   Module     â”‚   â”‚   Module     â”‚   â”‚   Modules    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                   â”‚
        â”‚                   â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼          â–¼                 â–¼
   /health          /hybrid-search   /ask          /ask-agentic
                                      /stream       /feedback
```

---

## ğŸ“ å®Œæ•´ API ç«¯ç‚¹åˆ—è¡¨

### 1. Health Check Module

#### `GET /api/v1/health`

**åŠŸèƒ½**: ç»¼åˆå¥åº·æ£€æŸ¥ï¼Œç”¨äºç›‘æ§å’Œè´Ÿè½½å‡è¡¡æ¢æµ‹

**å“åº”ç¤ºä¾‹**:
```json
{
  "status": "ok",
  "version": "0.1.0",
  "environment": "development",
  "service_name": "arxiv-paper-curator",
  "services": {
    "database": {
      "status": "healthy",
      "message": "Connected successfully"
    },
    "opensearch": {
      "status": "healthy",
      "message": "Index 'arxiv-papers-chunks' with 1523 documents"
    },
    "ollama": {
      "status": "healthy",
      "message": "Ollama running with 2 models loaded"
    }
  }
}
```

**æ£€æŸ¥çš„æœåŠ¡**:
- **PostgreSQL**: æ‰§è¡Œ `SELECT 1` éªŒè¯è¿æ¥
- **OpenSearch**: æ£€æŸ¥é›†ç¾¤å¥åº· + ç´¢å¼•ç»Ÿè®¡
- **Ollama**: å¼‚æ­¥å¥åº·æ£€æŸ¥ï¼ŒéªŒè¯ LLM å¯ç”¨æ€§

**è¿”å›çŠ¶æ€ç **:
- `200 OK`: æ‰€æœ‰æœåŠ¡æ­£å¸¸
- `200 OK` + `status: "degraded"`: éƒ¨åˆ†æœåŠ¡å¼‚å¸¸ï¼ˆä»å¯å“åº”ï¼‰

**ä½¿ç”¨åœºæ™¯**:
- Kubernetes liveness/readiness probes
- è´Ÿè½½å‡è¡¡å™¨å¥åº·æ£€æŸ¥
- è¿ç»´ç›‘æ§å‘Šè­¦
- å¼€å‘è°ƒè¯•éªŒè¯

---

### 2. Search Module

#### `POST /api/v1/hybrid-search/`

**åŠŸèƒ½**: æ··åˆæœç´¢ï¼ˆBM25 + Vectorï¼‰ï¼Œæ”¯æŒè‡ªåŠ¨é™çº§åˆ° BM25

**è¯·æ±‚ä½“**:
```json
{
  "query": "transformer attention mechanism",
  "size": 10,
  "from": 0,
  "categories": ["cs.AI", "cs.LG"],
  "latest_papers": false,
  "use_hybrid": true,
  "min_score": 0.0
}
```

**å‚æ•°è¯´æ˜**:
- `query` (å¿…å¡«): æœç´¢æŸ¥è¯¢æ–‡æœ¬ (1-500 å­—ç¬¦)
- `size`: è¿”å›ç»“æœæ•°é‡ (1-100, é»˜è®¤ 10)
- `from`: åˆ†é¡µåç§»é‡ (é»˜è®¤ 0)
- `categories`: arXiv åˆ†ç±»è¿‡æ»¤ï¼Œå¦‚ `["cs.AI", "cs.LG"]`
- `latest_papers`: æ˜¯å¦æŒ‰å‘å¸ƒæ—¥æœŸæ’åºï¼ˆå¦åˆ™æŒ‰ç›¸å…³æ€§ï¼‰
- `use_hybrid`: å¯ç”¨æ··åˆæœç´¢ï¼ˆBM25 + å‘é‡ï¼‰
- `min_score`: æœ€ä½åˆ†æ•°é˜ˆå€¼

**å“åº”ç¤ºä¾‹**:
```json
{
  "query": "transformer attention mechanism",
  "total": 45,
  "hits": [
    {
      "arxiv_id": "1706.03762",
      "title": "Attention is All You Need",
      "authors": "Ashish Vaswani, Noam Shazeer, ...",
      "abstract": "The dominant sequence transduction models...",
      "published_date": "2017-06-12",
      "pdf_url": "https://arxiv.org/pdf/1706.03762.pdf",
      "score": 15.234,
      "chunk_text": "The Transformer uses multi-head self-attention...",
      "chunk_id": "1706.03762_chunk_42",
      "section_name": "Model Architecture",
      "highlights": {
        "chunk_text": ["<em>Transformer</em> uses multi-head self-<em>attention</em>"]
      }
    }
  ],
  "size": 10,
  "from": 0,
  "search_mode": "hybrid"
}
```

**æœç´¢æ¨¡å¼è‡ªåŠ¨é€‰æ‹©**:
```python
# å†³ç­–é€»è¾‘
if use_hybrid and query_embedding_success:
    search_mode = "hybrid"  # BM25 + Vector (RRF èåˆ)
else:
    search_mode = "bm25"    # é™çº§åˆ°çº¯å…³é”®è¯æœç´¢
```

**æŠ€æœ¯å®ç°**:
- **BM25**: OpenSearch å…¨æ–‡æœç´¢ï¼ˆ`chunk_text` å­—æ®µï¼‰
- **Vector**: Jina Embeddings (1024ç»´) + HNSW ç´¢å¼•
- **RRF èåˆ**: OpenSearch åŸç”Ÿ Reciprocal Rank Fusion pipeline
- **åˆ†é¡µ**: ä½¿ç”¨ `from` + `size` å‚æ•°

**ä½¿ç”¨åœºæ™¯**:
- ç ”ç©¶è®ºæ–‡æŸ¥æ‰¾
- è¯­ä¹‰ç›¸ä¼¼æ–‡æ¡£æ£€ç´¢
- æ–‡çŒ®ç»¼è¿°å‡†å¤‡
- Gradio æœç´¢ç•Œé¢åç«¯

---

### 3. Basic RAG Module

#### `POST /api/v1/ask`

**åŠŸèƒ½**: åŸºç¡€ RAG é—®ç­”ï¼ˆéæµå¼ï¼‰ï¼Œå¸¦ç¼“å­˜å’Œè¿½è¸ª

**è¯·æ±‚ä½“**:
```json
{
  "query": "What are the advantages of transformers over RNNs?",
  "top_k": 3,
  "use_hybrid": true,
  "model": "llama3.2:1b",
  "categories": ["cs.AI"]
}
```

**å‚æ•°è¯´æ˜**:
- `query` (å¿…å¡«): ç”¨æˆ·é—®é¢˜ (1-1000 å­—ç¬¦)
- `top_k`: æ£€ç´¢æ–‡æ¡£å—æ•°é‡ (1-10, é»˜è®¤ 3)
- `use_hybrid`: ä½¿ç”¨æ··åˆæœç´¢ (é»˜è®¤ true)
- `model`: Ollama æ¨¡å‹åç§° (é»˜è®¤ "llama3.2:1b")
- `categories`: é™åˆ¶æœç´¢çš„ arXiv åˆ†ç±»

**å“åº”ç¤ºä¾‹**:
```json
{
  "query": "What are the advantages of transformers over RNNs?",
  "answer": "Based on the research papers, transformers have several key advantages over RNNs:\n\n1. **Parallelization**: Unlike RNNs which process sequences sequentially, transformers can process all tokens simultaneously through self-attention, enabling much faster training.\n\n2. **Long-range dependencies**: The attention mechanism allows direct connections between any two positions in the sequence, avoiding the vanishing gradient problem that affects RNNs.\n\n3. **Scalability**: Transformers scale better with model size and data, as demonstrated by GPT and BERT.\n\nSource: Attention is All You Need (Vaswani et al., 2017)",
  "sources": [
    "https://arxiv.org/pdf/1706.03762.pdf",
    "https://arxiv.org/pdf/1810.04805.pdf"
  ],
  "chunks_used": 3,
  "search_mode": "hybrid"
}
```

**å·¥ä½œæµç¨‹**:
```
1. æ£€æŸ¥ Redis ç¼“å­˜ (exact match)
   â”œâ”€ å‘½ä¸­ â†’ ç›´æ¥è¿”å›ç¼“å­˜ç»“æœ
   â””â”€ æœªå‘½ä¸­ â†’ ç»§ç»­å¤„ç†
2. ç”ŸæˆæŸ¥è¯¢ Embedding (å¦‚æœ use_hybrid=true)
3. OpenSearch æ··åˆæœç´¢ (top_k æ–‡æ¡£)
4. æ„å»º RAG Prompt
   â”œâ”€ ä½¿ç”¨ RAGPromptBuilder.create_structured_prompt()
   â””â”€ å¤±è´¥é™çº§åˆ° create_rag_prompt()
5. Ollama LLM ç”Ÿæˆç­”æ¡ˆ
6. å­˜å‚¨ç»“æœåˆ° Redis ç¼“å­˜
7. è¿”å›å“åº”
```

**Langfuse è¿½è¸ª**:
- Trace çº§åˆ«: å®Œæ•´è¯·æ±‚
- Span å±‚æ¬¡:
  - `embedding`: æŸ¥è¯¢å‘é‡åŒ–
  - `search`: æ–‡æ¡£æ£€ç´¢
  - `prompt_construction`: Prompt æ„å»º
  - `generation`: LLM ç”Ÿæˆ

**ç¼“å­˜ç­–ç•¥**:
- **Key**: Hash of `(query, top_k, use_hybrid, model, categories)`
- **TTL**: 6 å°æ—¶ (å¯é…ç½® `REDIS__TTL_HOURS`)
- **å­˜å‚¨**: Redis (exact match only)

**é”™è¯¯å¤„ç†**:
- `500`: æœç´¢å¤±è´¥ã€LLM é”™è¯¯ã€ç³»ç»Ÿå¼‚å¸¸
- æ— ç›¸å…³æ–‡æ¡£: è¿”å› "I couldn't find any relevant information..."

**ä½¿ç”¨åœºæ™¯**:
- Web åº”ç”¨é—®ç­”æ¥å£
- æ‰¹é‡æŸ¥è¯¢å¤„ç†
- API é›†æˆï¼ˆéå®æ—¶å¯¹è¯ï¼‰

---

#### `POST /api/v1/stream`

**åŠŸèƒ½**: æµå¼ RAG é—®ç­”ï¼ˆServer-Sent Eventsï¼‰

**è¯·æ±‚ä½“**: ä¸ `/ask` ç›¸åŒ

**å“åº”æ ¼å¼** (SSE):
```
data: {"sources": ["https://arxiv.org/pdf/1706.03762.pdf"], "chunks_used": 3, "search_mode": "hybrid"}

data: {"chunk": "Based "}

data: {"chunk": "on "}

data: {"chunk": "the "}

data: {"chunk": "research "}

data: {"chunk": "papers, "}

...

data: {"answer": "Based on the research papers, transformers have...", "done": true}
```

**SSE äº‹ä»¶åºåˆ—**:
1. **Metadata äº‹ä»¶** (é¦–ä¸ª): åŒ…å« `sources`, `chunks_used`, `search_mode`
2. **Chunk äº‹ä»¶** (å¤šä¸ª): æ¯æ¬¡ LLM ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ `{"chunk": "..."}`
3. **Done äº‹ä»¶** (æœ€å): å®Œæ•´ç­”æ¡ˆ + `done: true` æ ‡å¿—

**å·¥ä½œæµç¨‹**:
```
1. æ£€æŸ¥ Redis ç¼“å­˜
   â”œâ”€ å‘½ä¸­ â†’ æµå¼å‘é€ç¼“å­˜å†…å®¹ï¼ˆæ¨¡æ‹Ÿæµå¼ï¼‰
   â””â”€ æœªå‘½ä¸­ â†’ ç»§ç»­å¤„ç†
2. æ£€ç´¢æ–‡æ¡£ (ä¸ /ask ç›¸åŒ)
3. å‘é€ Metadata äº‹ä»¶
4. æ„å»º Prompt
5. Ollama æµå¼ç”Ÿæˆ
   â””â”€ æ¯ä¸ª token â†’ ç«‹å³å‘é€ chunk äº‹ä»¶
6. å‘é€ Done äº‹ä»¶ï¼ˆåŒ…å«å®Œæ•´ç­”æ¡ˆï¼‰
7. å­˜å‚¨åˆ° Redis ç¼“å­˜
```

**ç¼“å­˜æµå¼æ’­æ”¾**:
```python
# ç¼“å­˜å‘½ä¸­æ—¶ï¼Œå°†å®Œæ•´ç­”æ¡ˆæ‹†åˆ†ä¸ºå•è¯æµå¼å‘é€
for chunk in cached_response.answer.split():
    yield f"data: {json.dumps({'chunk': chunk + ' '})}\n\n"
```

**Headers**:
```
Content-Type: text/plain
Cache-Control: no-cache
Connection: keep-alive
```

**é”™è¯¯å¤„ç†**:
```
data: {"error": "Search service unavailable"}
```

**ä½¿ç”¨åœºæ™¯**:
- Gradio èŠå¤©ç•Œé¢
- å®æ—¶å¯¹è¯ä½“éªŒ
- WebSocket ä¸å¯ç”¨æ—¶çš„æ›¿ä»£æ–¹æ¡ˆ
- å‰ç«¯æ‰“å­—æœºæ•ˆæœ

**å¯¹æ¯” /ask**:
| ç‰¹æ€§ | /ask | /stream |
|------|------|---------|
| å“åº”ç±»å‹ | JSON | Server-Sent Events |
| é¦–å­—èŠ‚æ—¶é—´ | å®Œæ•´ç”Ÿæˆå | ç«‹å³å¼€å§‹ |
| ç”¨æˆ·ä½“éªŒ | ç­‰å¾…å®Œæ•´ç­”æ¡ˆ | å®æ—¶æµå¼æ˜¾ç¤º |
| ç¼“å­˜è¡Œä¸º | ç›´æ¥è¿”å› | æ¨¡æ‹Ÿæµå¼æ’­æ”¾ |
| å‰ç«¯å¤æ‚åº¦ | ä½ | ä¸­ç­‰ï¼ˆSSE å¤„ç†ï¼‰ |

---

### 4. Agentic RAG Module

#### `POST /api/v1/ask-agentic`

**åŠŸèƒ½**: æ™ºèƒ½ RAG ç³»ç»Ÿï¼ˆLangGraphï¼‰ï¼Œå…·å¤‡å†³ç­–èƒ½åŠ›å’Œè‡ªé€‚åº”æ£€ç´¢

**è¯·æ±‚ä½“**: ä¸ `/ask` ç›¸åŒ

**å“åº”ç¤ºä¾‹**:
```json
{
  "query": "What are the latest developments in quantum computing?",
  "answer": "Based on recent research papers in quantum computing, key developments include:\n\n1. Quantum Error Correction: Recent papers demonstrate improved error rates using surface codes...\n\n2. Quantum Algorithms: Novel algorithms for optimization problems showing quantum advantage...\n\n[Citations: Nature 2023, Science 2024]",
  "sources": [
    "https://arxiv.org/pdf/2301.12345.pdf",
    "https://arxiv.org/pdf/2302.67890.pdf"
  ],
  "chunks_used": 3,
  "search_mode": "hybrid",
  "reasoning_steps": [
    "âœ“ Query validation: Scope check passed (score: 85/100)",
    "âœ“ Document retrieval: Retrieved 3 candidate chunks",
    "âœ“ Relevance grading: 3/3 chunks marked as relevant",
    "âœ“ Answer generation: Generated response from relevant context"
  ],
  "retrieval_attempts": 1,
  "trace_id": "langfuse-trace-abc123-def456"
}
```

**å…³é”®å¢å¼º**:
- `reasoning_steps`: é€æ˜çš„å†³ç­–è¿‡ç¨‹
- `retrieval_attempts`: è‡ªé€‚åº”æ£€ç´¢æ¬¡æ•°ï¼ˆ1-2ï¼‰
- `trace_id`: Langfuse è¿½è¸ª IDï¼ˆç”¨äºåé¦ˆï¼‰

**LangGraph å·¥ä½œæµ**:

```mermaid
graph TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B[Guardrail Node]
    B -->|score >= 60| C[Retrieve Node]
    B -->|score < 60| Z[Out of Scope]
    C --> D[Tool Node - æ‰§è¡Œæ£€ç´¢]
    D --> E[Grade Documents Node]
    E -->|æœ‰ç›¸å…³æ–‡æ¡£| F[Generate Answer Node]
    E -->|æ— ç›¸å…³æ–‡æ¡£| G{é‡è¯•æ¬¡æ•° < 2?}
    G -->|æ˜¯| H[Rewrite Query Node]
    H --> C
    G -->|å¦| I[Generate Fallback]
    F --> J[æœ€ç»ˆç­”æ¡ˆ]
    I --> J
```

**èŠ‚ç‚¹è¯¦ç»†è¯´æ˜**:

1. **Guardrail Node** (æŸ¥è¯¢éªŒè¯)
   - **åŠŸèƒ½**: æ£€æµ‹æŸ¥è¯¢æ˜¯å¦å±äºå­¦æœ¯ç ”ç©¶èŒƒå›´
   - **LLM è°ƒç”¨**: æ˜¯
   - **è¾“å‡º**: `GuardrailScoring` (score: 0-100, reason: str)
   - **é˜ˆå€¼**: 60 åˆ†ï¼ˆå¯é…ç½® `guardrail_threshold`ï¼‰
   - **ç¤ºä¾‹åˆ¤æ–­**:
     - âœ“ "What are transformers?" â†’ 85 åˆ†
     - âœ— "What's the weather?" â†’ 20 åˆ†

2. **Retrieve Node** (æ£€ç´¢åè°ƒ)
   - **åŠŸèƒ½**: åˆ›å»º LangChain Tool è°ƒç”¨è¯·æ±‚
   - **LLM è°ƒç”¨**: å¦ï¼ˆä»…ç”Ÿæˆ tool callï¼‰
   - **è¾“å‡º**: Tool invocation message

3. **Tool Node** (å®é™…æ£€ç´¢)
   - **åŠŸèƒ½**: æ‰§è¡Œ OpenSearch æ··åˆæœç´¢
   - **è°ƒç”¨**: `retrieve_papers` LangChain Tool
   - **è¿”å›**: `List[Document]` (LangChain æ ¼å¼)

4. **Grade Documents Node** (æ–‡æ¡£è¯„åˆ†)
   - **åŠŸèƒ½**: LLM è¯„ä¼°æ¯ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§
   - **LLM è°ƒç”¨**: æ˜¯
   - **è¾“å‡º**: `GradeDocuments` (binary_score: yes/no, relevant_count: int)
   - **é€»è¾‘**:
     ```python
     relevant_count = sum(1 for doc in docs if grading_llm.grade(doc) == "yes")
     if relevant_count > 0:
         proceed_to_generation()
     else:
         rewrite_query()
     ```

5. **Rewrite Query Node** (æŸ¥è¯¢ä¼˜åŒ–)
   - **åŠŸèƒ½**: LLM é‡å†™æŸ¥è¯¢ä»¥æé«˜æ£€ç´¢è´¨é‡
   - **LLM è°ƒç”¨**: æ˜¯
   - **è§¦å‘æ¡ä»¶**: æ— ç›¸å…³æ–‡æ¡£ + é‡è¯•æ¬¡æ•° < 2
   - **ç¤ºä¾‹**:
     - åŸæŸ¥è¯¢: "What is attention?"
     - é‡å†™å: "What is the attention mechanism in transformer neural networks?"

6. **Generate Answer Node** (ç­”æ¡ˆç”Ÿæˆ)
   - **åŠŸèƒ½**: åŸºäºç›¸å…³æ–‡æ¡£ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
   - **LLM è°ƒç”¨**: æ˜¯
   - **Prompt**: åŒ…å«æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸Šä¸‹æ–‡
   - **è¾“å‡º**: æœ€ç»ˆç­”æ¡ˆå­—ç¬¦ä¸²

**çŠ¶æ€ç®¡ç†** (AgentState):
```python
{
    "messages": [HumanMessage, AIMessage, ...],  # å¯¹è¯å†å²
    "guardrail_result": GuardrailScoring,        # éªŒè¯ç»“æœ
    "relevant_sources": List[Document],          # ç›¸å…³æ–‡æ¡£
    "retrieval_attempts": int,                   # é‡è¯•æ¬¡æ•°
    "reasoning_steps": List[str]                 # æ¨ç†è¿‡ç¨‹
}
```

**é…ç½®å‚æ•°** (GraphConfig):
- `max_retrieval_attempts`: æœ€å¤§æ£€ç´¢é‡è¯•æ¬¡æ•° (é»˜è®¤ 2)
- `guardrail_threshold`: æŸ¥è¯¢éªŒè¯é˜ˆå€¼ (é»˜è®¤ 60)
- `model`: LLM æ¨¡å‹ (é»˜è®¤ "llama3.2:1b")
- `temperature`: ç”Ÿæˆæ¸©åº¦ (é»˜è®¤ 0.0)
- `top_k`: æ£€ç´¢æ–‡æ¡£æ•°é‡ (é»˜è®¤ 3)

**Langfuse å®Œæ•´è¿½è¸ª**:
```
Trace: agentic_rag_request_abc123
â”œâ”€ Span: guardrail_validation (score: 85)
â”œâ”€ Span: document_retrieval (3 docs)
â”œâ”€ Span: document_grading (3/3 relevant)
â””â”€ Span: answer_generation (model: llama3.2:1b)
```

**é”™è¯¯å¤„ç†**:
- `422 Unprocessable Entity`: æŸ¥è¯¢éªŒè¯å¤±è´¥ï¼ˆscope å¤–ï¼‰
- `500 Internal Server Error`: LLM è°ƒç”¨å¤±è´¥ã€å·¥ä½œæµå¼‚å¸¸

**å¯¹æ¯” /ask**:
| ç‰¹æ€§ | /ask | /ask-agentic |
|------|------|--------------|
| æŸ¥è¯¢éªŒè¯ | âœ— | âœ“ Guardrail |
| æ–‡æ¡£è¯„åˆ† | âœ— | âœ“ LLM Grading |
| æŸ¥è¯¢é‡å†™ | âœ— | âœ“ è‡ªåŠ¨ä¼˜åŒ– |
| æ¨ç†é€æ˜åº¦ | âœ— | âœ“ reasoning_steps |
| LLM è°ƒç”¨æ¬¡æ•° | 1 | 3-5 æ¬¡ |
| å“åº”æ—¶é—´ | ~2-3s | ~5-8s |
| å‡†ç¡®æ€§ | ä¸­ç­‰ | é«˜ |
| æˆæœ¬ | ä½ | ä¸­ç­‰ |

**ä½¿ç”¨åœºæ™¯**:
- éœ€è¦é«˜å‡†ç¡®æ€§çš„ç ”ç©¶æŸ¥è¯¢
- å¤æ‚/æ¨¡ç³Šé—®é¢˜çš„æ™ºèƒ½å¤„ç†
- éœ€è¦å®¡è®¡è¿½è¸ªçš„ä¼ä¸šåº”ç”¨
- è‡ªåŠ¨æŸ¥è¯¢ä¼˜åŒ–éœ€æ±‚

---

#### `POST /api/v1/feedback`

**åŠŸèƒ½**: æäº¤ç”¨æˆ·åé¦ˆåˆ° Langfuseï¼Œç”¨äºæŒç»­æ”¹è¿›

**è¯·æ±‚ä½“**:
```json
{
  "trace_id": "langfuse-trace-abc123-def456",
  "score": 1.0,
  "comment": "éå¸¸å‡†ç¡®çš„å›ç­”ï¼Œå¼•ç”¨äº†ç›¸å…³è®ºæ–‡ï¼"
}
```

**å‚æ•°è¯´æ˜**:
- `trace_id` (å¿…å¡«): ä» `/ask-agentic` å“åº”ä¸­è·å–
- `score` (å¿…å¡«): åé¦ˆåˆ†æ•° (-1.0 åˆ° 1.0)
  - `1.0`: éå¸¸æ»¡æ„
  - `0.0`: ä¸­æ€§
  - `-1.0`: éå¸¸ä¸æ»¡æ„
- `comment` (å¯é€‰): æ–‡å­—åé¦ˆ (æœ€å¤š 1000 å­—ç¬¦)

**å“åº”ç¤ºä¾‹**:
```json
{
  "success": true,
  "message": "Feedback recorded successfully"
}
```

**Langfuse é›†æˆ**:
```python
# å†…éƒ¨å®ç°
langfuse_tracer.submit_feedback(
    trace_id=request.trace_id,
    score=request.score,
    comment=request.comment
)
langfuse_tracer.flush()  # ç«‹å³å‘é€åˆ° Langfuse æœåŠ¡å™¨
```

**Langfuse ä»ªè¡¨ç›˜æ•ˆæœ**:
- åé¦ˆä¸åŸå§‹ trace å…³è”
- å¯æŒ‰åˆ†æ•°ç­›é€‰æŸ¥è¯¢
- ç”Ÿæˆè´¨é‡è¶‹åŠ¿å›¾
- å¯¼å‡ºä½åˆ† trace ç”¨äºè°ƒè¯•

**é”™è¯¯å¤„ç†**:
- `503 Service Unavailable`: Langfuse æœªå¯ç”¨
- `500 Internal Server Error`: æäº¤å¤±è´¥

**ä½¿ç”¨åœºæ™¯**:
- Gradio ç•Œé¢çš„ ğŸ‘/ğŸ‘ æŒ‰é’®
- A/B æµ‹è¯•è¯„ä¼°
- æ¨¡å‹è´¨é‡ç›‘æ§
- ç”¨æˆ·æ»¡æ„åº¦åˆ†æ

**åé¦ˆé—­ç¯**:
```
ç”¨æˆ·æŸ¥è¯¢ â†’ /ask-agentic â†’ è¿”å› trace_id
    â†“
ç”¨æˆ·è¯„ä»· â†’ /feedback â†’ Langfuse å­˜å‚¨
    â†“
æ•°æ®åˆ†æ â†’ è¯†åˆ«é—®é¢˜ â†’ ä¼˜åŒ– Prompt/æ¨¡å‹
    â†“
éƒ¨ç½²æ”¹è¿› â†’ ç”¨æˆ·æŸ¥è¯¢ï¼ˆå¾ªç¯ï¼‰
```

---

## ğŸ”„ API è°ƒç”¨æµç¨‹å›¾

### åŸºç¡€ RAG æµç¨‹ (/ask)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ POST /api/v1/ask
     â”‚ {"query": "...", "top_k": 3}
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ask Router     â”‚
â”‚  (ask.py)       â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â”€â”€ 1. Check Redis Cache â”€â”€â”€â”€â”€â”
     â”‚                               â”‚
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
     â”‚    â”‚  Cache Client   â”‚        â”‚
     â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
     â”‚         â”‚ Hit? â†’ Return       â”‚
     â”‚         â”‚ Miss? â†’ Continue    â”‚
     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â”€â”€ 2. Generate Embedding â”€â”€â”€â”€â”
     â”‚                               â”‚
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
     â”‚    â”‚ Jina Embeddings â”‚        â”‚
     â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
     â”‚         â”‚ 1024-dim vector     â”‚
     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â”€â”€ 3. Hybrid Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                               â”‚
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
     â”‚    â”‚   OpenSearch    â”‚        â”‚
     â”‚    â”‚  (BM25 + Vector)â”‚        â”‚
     â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
     â”‚         â”‚ Top-K Chunks        â”‚
     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â”€â”€ 4. Build RAG Prompt â”€â”€â”€â”€â”€â”€â”
     â”‚                               â”‚
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
     â”‚    â”‚ RAGPromptBuilderâ”‚        â”‚
     â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
     â”‚         â”‚ Structured Prompt   â”‚
     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â”€â”€ 5. LLM Generation â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                               â”‚
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
     â”‚    â”‚  Ollama Client  â”‚        â”‚
     â”‚    â”‚ (llama3.2:1b)   â”‚        â”‚
     â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
     â”‚         â”‚ Generated Answer    â”‚
     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â””â”€â”€â”€ 6. Store in Cache â†’ Return
          â”‚
          â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Response â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agentic RAG æµç¨‹ (/ask-agentic)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ POST /api/v1/ask-agentic
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agentic RAG Service    â”‚
â”‚ (LangGraph Workflow)   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangGraph State Machine            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  1. [Guardrail Node]                â”‚
â”‚     â”œâ”€ LLM è¯„ä¼°æŸ¥è¯¢èŒƒå›´             â”‚
â”‚     â”œâ”€ Score >= 60? â†’ Continue      â”‚
â”‚     â””â”€ Score < 60? â†’ Out of Scope   â”‚
â”‚                                     â”‚
â”‚  2. [Retrieve Node]                 â”‚
â”‚     â””â”€ åˆ›å»º Tool Call               â”‚
â”‚                                     â”‚
â”‚  3. [Tool Node]                     â”‚
â”‚     â””â”€ OpenSearch æ£€ç´¢              â”‚
â”‚                                     â”‚
â”‚  4. [Grade Documents Node]          â”‚
â”‚     â”œâ”€ LLM è¯„ä¼°æ¯ä¸ªæ–‡æ¡£ç›¸å…³æ€§       â”‚
â”‚     â”œâ”€ Relevant > 0? â†’ Generate     â”‚
â”‚     â””â”€ Relevant = 0? â†’ Rewrite      â”‚
â”‚                                     â”‚
â”‚  5a. [Rewrite Query Node]           â”‚
â”‚      â”œâ”€ LLM ä¼˜åŒ–æŸ¥è¯¢                â”‚
â”‚      â””â”€ Attempts < 2? â†’ Retry (2)   â”‚
â”‚                                     â”‚
â”‚  5b. [Generate Answer Node]         â”‚
â”‚      â””â”€ LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ            â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Response â”‚
      â”‚ + trace_idâ”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š æ•°æ®æµå‘åˆ†æ

### æœåŠ¡ä¾èµ–å…³ç³»
```
FastAPI Routers
    â”‚
    â”œâ”€â”€â”€ ping.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â†’ PostgreSQL (Database)
    â”‚                      â”œâ”€â†’ OpenSearch
    â”‚                      â””â”€â†’ Ollama
    â”‚
    â”œâ”€â”€â”€ hybrid_search.py â”€â”¬â”€â†’ OpenSearch (BM25 + Vector)
    â”‚                      â””â”€â†’ Jina Embeddings
    â”‚
    â”œâ”€â”€â”€ ask.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â†’ OpenSearch
    â”‚                      â”œâ”€â†’ Jina Embeddings
    â”‚                      â”œâ”€â†’ Ollama
    â”‚                      â”œâ”€â†’ Langfuse (Tracing)
    â”‚                      â””â”€â†’ Redis (Cache)
    â”‚
    â””â”€â”€â”€ agentic_ask.py â”€â”€â”€â”¬â”€â†’ AgenticRAGService
                           â”‚   â”œâ”€â†’ OpenSearch
                           â”‚   â”œâ”€â†’ Jina Embeddings
                           â”‚   â”œâ”€â†’ Ollama (å¤šæ¬¡è°ƒç”¨)
                           â”‚   â””â”€â†’ Langfuse (å®Œæ•´è¿½è¸ª)
                           â””â”€â†’ Langfuse (Feedback)
```

### å…±äº«çŠ¶æ€ (app.state)
```python
# main.py lifespan åˆå§‹åŒ–
app.state.settings          # Pydantic Settings
app.state.database          # PostgreSQL Session Factory
app.state.opensearch_client # OpenSearch Client
app.state.arxiv_client      # arXiv API Client
app.state.pdf_parser        # Docling PDF Parser
app.state.embeddings_service# Jina Embeddings Client
app.state.ollama_client     # Ollama LLM Client
app.state.langfuse_tracer   # Langfuse Tracer
app.state.cache_client      # Redis Cache Client
app.state.telegram_service  # Telegram Bot (Week 7)
```

---

## ğŸ¯ ä½¿ç”¨å»ºè®®

### 1. é€‰æ‹©åˆé€‚çš„ç«¯ç‚¹

| åœºæ™¯ | æ¨èç«¯ç‚¹ | åŸå›  |
|------|---------|------|
| ç®€å•é—®ç­” | `/ask` | å¿«é€Ÿã€æœ‰ç¼“å­˜ |
| å®æ—¶èŠå¤© | `/stream` | æµå¼ä½“éªŒå¥½ |
| å¤æ‚ç ”ç©¶æŸ¥è¯¢ | `/ask-agentic` | æ™ºèƒ½æ£€ç´¢ã€é«˜å‡†ç¡®æ€§ |
| æ–‡æ¡£æ£€ç´¢ | `/hybrid-search` | ç›´æ¥æœç´¢ã€æ”¯æŒåˆ†é¡µ |
| ç³»ç»Ÿç›‘æ§ | `/health` | å…¨é¢å¥åº·æ£€æŸ¥ |

### 2. æ€§èƒ½ä¼˜åŒ–

**ç¼“å­˜ç­–ç•¥**:
```python
# ç›¸åŒæŸ¥è¯¢ â†’ å‘½ä¸­ç¼“å­˜ï¼ˆRedisï¼‰
request_1 = {"query": "What is attention?", "top_k": 3, "use_hybrid": true}
request_2 = {"query": "What is attention?", "top_k": 3, "use_hybrid": true}
# â†’ request_2 ç›´æ¥ä»ç¼“å­˜è¿”å›ï¼Œå»¶è¿Ÿ < 50ms

# ä¸åŒå‚æ•° â†’ ç¼“å­˜æœªå‘½ä¸­
request_3 = {"query": "What is attention?", "top_k": 5, "use_hybrid": true}
# â†’ å®Œæ•´å¤„ç†æµç¨‹
```

**å¹¶å‘é™åˆ¶**:
- Ollama å¹¶å‘æ•°: å–å†³äº GPU èµ„æº
- OpenSearch å¹¶å‘: æ— é™åˆ¶ï¼ˆé›†ç¾¤æ”¯æŒï¼‰
- Redis è¿æ¥æ± : 10 è¿æ¥ï¼ˆå¯é…ç½®ï¼‰

### 3. é”™è¯¯å¤„ç†æœ€ä½³å®è·µ

```python
# å®¢æˆ·ç«¯ç¤ºä¾‹
import httpx
import asyncio

async def robust_ask(query: str, max_retries: int = 3):
    """å¸¦é‡è¯•çš„ RAG æŸ¥è¯¢"""
    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:8000/api/v1/ask",
                    json={"query": query, "top_k": 3},
                    timeout=30.0
                )
                response.raise_for_status()
                return response.json()
        except httpx.TimeoutException:
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                continue
            raise
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 503:
                # æœåŠ¡ä¸å¯ç”¨ â†’ é‡è¯•
                await asyncio.sleep(5)
                continue
            raise
```

### 4. Langfuse è¿½è¸ªæœ€ä½³å®è·µ

```python
# 1. ä½¿ç”¨ /ask-agentic è·å– trace_id
response = await client.post("/api/v1/ask-agentic", json={...})
trace_id = response.json()["trace_id"]

# 2. ç”¨æˆ·åé¦ˆ
await client.post("/api/v1/feedback", json={
    "trace_id": trace_id,
    "score": 1.0,  # ç”¨æˆ·ç‚¹èµ
    "comment": "Very helpful!"
})

# 3. Langfuse ä»ªè¡¨ç›˜æŸ¥çœ‹
# http://localhost:3000 â†’ Traces â†’ æœç´¢ trace_id
```

---

## ğŸ”§ å¼€å‘å’Œæµ‹è¯•

### cURL ç¤ºä¾‹

```bash
# 1. å¥åº·æ£€æŸ¥
curl http://localhost:8000/api/v1/health | jq

# 2. æ··åˆæœç´¢
curl -X POST http://localhost:8000/api/v1/hybrid-search/ \
  -H "Content-Type: application/json" \
  -d '{
    "query": "transformer architecture",
    "size": 5,
    "use_hybrid": true
  }' | jq

# 3. åŸºç¡€é—®ç­”
curl -X POST http://localhost:8000/api/v1/ask \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are transformers?",
    "top_k": 3,
    "use_hybrid": true
  }' | jq

# 4. æµå¼é—®ç­”
curl -X POST http://localhost:8000/api/v1/stream \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Explain attention mechanism",
    "top_k": 3
  }'

# 5. Agentic RAG
curl -X POST http://localhost:8000/api/v1/ask-agentic \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Latest developments in quantum computing",
    "top_k": 3
  }' | jq

# 6. æäº¤åé¦ˆ
curl -X POST http://localhost:8000/api/v1/feedback \
  -H "Content-Type: application/json" \
  -d '{
    "trace_id": "langfuse-trace-abc123",
    "score": 1.0,
    "comment": "Excellent answer!"
  }' | jq
```

### Python å®¢æˆ·ç«¯ç¤ºä¾‹

```python
import httpx
import asyncio

async def main():
    async with httpx.AsyncClient() as client:
        # Agentic RAG æŸ¥è¯¢
        response = await client.post(
            "http://localhost:8000/api/v1/ask-agentic",
            json={
                "query": "What is self-attention in transformers?",
                "top_k": 3,
                "use_hybrid": True
            }
        )
        result = response.json()

        print(f"Answer: {result['answer']}")
        print(f"\nReasoning Steps:")
        for step in result['reasoning_steps']:
            print(f"  - {step}")

        print(f"\nSources:")
        for source in result['sources']:
            print(f"  - {source}")

asyncio.run(main())
```

---

## ğŸ“ˆ ç›‘æ§å’Œå¯è§‚æµ‹æ€§

### Langfuse ä»ªè¡¨ç›˜
- **URL**: http://localhost:3000
- **åŠŸèƒ½**:
  - æŸ¥çœ‹æ‰€æœ‰ API è°ƒç”¨ trace
  - åˆ†æ LLM è°ƒç”¨æˆæœ¬
  - æŸ¥çœ‹ç”¨æˆ·åé¦ˆ
  - æ€§èƒ½ç“¶é¢ˆåˆ†æ

### OpenSearch Dashboards
- **URL**: http://localhost:5601
- **åŠŸèƒ½**:
  - æŸ¥çœ‹ç´¢å¼•ç»Ÿè®¡
  - è°ƒè¯•æœç´¢æŸ¥è¯¢
  - åˆ†ææœç´¢æ€§èƒ½

### æ—¥å¿—åˆ†æ
```bash
# æŸ¥çœ‹ API æ—¥å¿—
docker compose logs api -f

# æŸ¥çœ‹ Ollama æ—¥å¿—
docker compose logs ollama -f

# æŸ¥çœ‹ OpenSearch æ—¥å¿—
docker compose logs opensearch -f
```

---

## ğŸš€ æ€»ç»“

### API åŠŸèƒ½çŸ©é˜µ

| ç«¯ç‚¹ | ç¼“å­˜ | æµå¼ | æ™ºèƒ½æ£€ç´¢ | Langfuse | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|----------|---------|
| `/health` | âœ— | âœ— | âœ— | âœ— | ç›‘æ§ |
| `/hybrid-search` | âœ— | âœ— | âœ— | âœ— | æ–‡æ¡£æŸ¥æ‰¾ |
| `/ask` | âœ“ | âœ— | âœ— | âœ“ | å¿«é€Ÿé—®ç­” |
| `/stream` | âœ“ | âœ“ | âœ— | âœ“ | å®æ—¶èŠå¤© |
| `/ask-agentic` | âœ— | âœ— | âœ“ | âœ“ | å¤æ‚ç ”ç©¶ |
| `/feedback` | âœ— | âœ— | âœ— | âœ“ | è´¨é‡æ”¹è¿› |

### æŠ€æœ¯æ ˆæ€»è§ˆ
- **API æ¡†æ¶**: FastAPI + Uvicorn
- **æœç´¢å¼•æ“**: OpenSearch (BM25 + Vector + RRF)
- **å‘é‡æ¨¡å‹**: Jina Embeddings (1024ç»´)
- **LLM**: Ollama (llama3.2:1b)
- **Agent æ¡†æ¶**: LangGraph + LangChain
- **ç¼“å­˜**: Redis (6 å°æ—¶ TTL)
- **è¿½è¸ª**: Langfuse v3
- **æ•°æ®åº“**: PostgreSQL

### æ€§èƒ½åŸºå‡†
- **å¥åº·æ£€æŸ¥**: < 100ms
- **æ··åˆæœç´¢**: 200-500ms
- **åŸºç¡€ RAG**: 2-4s (é¦–æ¬¡) / < 100ms (ç¼“å­˜)
- **æµå¼ RAG**: é¦–å­—èŠ‚ < 500ms
- **Agentic RAG**: 5-10s (3-5 æ¬¡ LLM è°ƒç”¨)
